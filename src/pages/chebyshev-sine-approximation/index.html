{% from 'base.html' import heading, offsite_link, show_image %}
{{ page.set_title("Approximating sin(x) to 5 ULP with Chebyshev polynomials") }}

{% extends 'blog_entry.html' %}

{% block entry_content %}

{{ multiline_code("""
fn sine(x: f32) -> f32 {
    let coeffs = [
        -0.101321183346709072589001712988183609230944236760490476f32, // x
         0.00662087952180793343258682906697112938547424931632185616f32, // x^3
        -0.000173505057912483501491115906801116298084629719204655552f32, // x^5
         2.52229235749396866288379170129828403876289663605034418e-6f32, // x^7
        -2.33177897192836082466066115718536782354224647348350113e-8f32, // x^9
         1.32913446369766718120324917415992976452154154051525892e-10f32, // x^11
    ];
    let x2 = x*x;
    let p11 = coeffs[5];
    let p9  = p11*x2 + coeffs[4];
    let p7  = p9*x2  + coeffs[3];
    let p5  = p7*x2  + coeffs[2];
    let p3  = p5*x2  + coeffs[1];
    let p1  = p3*x2  + coeffs[0];
    (x - 3.1415927410125732f32 + 0.00000008742277657347586f32) *
    (x + 3.1415927410125732f32 - 0.00000008742277657347586f32) * p1 * x
}
""") }}
<p>
That code above will compute the sine of any IEEE single-precision float in the range of (-Pi, Pi)
to within 6 Units of Least Precision ({{ offsite_link("https://en.wikipedia.org/wiki/Unit_in_the_last_place", "ULP") }}).
A ULP is a useful way to measure relative error in a floating point computation. Given two adjacent floating point numbers, their difference
in terms of ULP is essentially equivalent to the number of floats that lie between them (plus one).
</p>

<p>
Given that IEEE single-precision floats have 24 bits of precision, this sine function is accurate to within 0.000036% (i.e. 6/2^24).
Achieving this precision over a domain <em>this</em> large was a challenge for me.
Many polynomial approximations to sin(x) operate only over (-Pi/4, Pi/4) or (-Pi/2, Pi/2).
These methods require modifications to work with larger ranges that I had not seen published before,
so the remainder of this article will discuss my approach to deriving the sine(x) function above.
</p>

{{ heading("Sin(x) as a sum of Chebyshev polynomials") }}

<p>
The first step is to re-express sin(x) over the domain of interest as an infinite polynomial.
One could use a Taylor series, but convergence is very slow.
Instead, we use Chebyshev polynomials.
</p>

<p>
For me, it helps to view these things using concepts from linear algebra.
Any continuous and smooth function can be expressed as a polynomial.
P(x) = a_0 + a_1 x + a_2 x^2 + ...

This is a weighted sum of powers of x, and because {1, x, x^2, ...} are all orthogonal,
we can think of this set as a <em>basis</em> for our function.
</p>

<p>
Instead of representing our function in terms of the most familiar basis for polynomials ({1, x, x^2, ...}),
we can instead use {T_0(x), T_1(x), T_2(x), ...}, where T_n(x) is the
{{ offsite_link("https://en.wikipedia.org/wiki/Chebyshev_polynomials#First_kind", "Chebyshev polynomial of the first kind") }}.
</p>

{{ show_image(page.images["chebyshev_basis.svg"], caption="The first five Chebyshev polynomials. From Wikipedia (public domain).") }}

<p>
Chebyshev polynomials have the property that all their local extrema lie within -1.0 < x < 1.0, and each such extrema
is at exactly +/- 1.
For those familiar with Fourier series, this should look somewhat familiar. We're projecting our sin(x) function onto
a basis where each basis function is oscillating at an increasing rate.
It's intuitive, then, that the smoother our function is, the more rapidly the coefficients of its projection
onto {T_0(x), T_1(x), ...} will decay (e.g. sin(x) doesn't oscillate rapidly over the interval (-1, 1), so any
rapidly-oscillating T_n(x) component would have to be small).

Although our basis has infinite cardinality, most energy is located in the lower components.
We can truncate the basis to {T_0(x), ... T_N(x)} for a relatively small value of N and achieve
a <em>very</em> good approximation of sin(x) over the interval (-1.0, 1.0).
</p>

<p>
Notice that for a function approximated with a finite set of Chebyshev polynomials,
the error is spread in a fairly uniform manner. The approximation isn't intrinsically more accurate
near 0 and less accurate at the extremeties, for example. We can say that
|E(x)| < B for all -1 < x < 1, where E(x) is the error of our approximation function (E(x) = sin^(x) - sin(x)),
and B is some bound that decreases as we increase the number of terms in our approximation.
</p>

<p>
This is good. If we approximated sin(x) with 6 chebyshev terms, we might well get
the error bound, B, down to 10^-9. However, optimizing for <em>absolute</em> error is generally
not what we want! The nature of floating point numbers is such that precision
is as much as 10^-45 near zero, and as little as 10^-8 near one.
What we really want to minimize is the <em>relative error</em>. As long as
|E(x) / sin(x)| < 2^-24, we know that there is no closer float to the true value than what
we've approximated*.
</p>

<p>
* The catch is that achieving |E(x) / sin(x)| < 2^-24 only ensures we're off by less than 1 ULP.
If the true answer falls in between two floats, this isn't enough to guarantee that we round to the
correct one. If the answer is nearly the average of the two adjacent floats,
being off by just 0.01 ULP in our model could cause incorrect rounding that actually bumps us to a
full 0.5 ULP error. I imagine optimizing for 0.5 ULP error requires more novel techniques.
</p>

{# Even intel only advertizes something like 0.56 ULP, right? Would be an interesting aside. #}

{{ heading("Optimizing relative error") }}
<p>
To optimize for relative error, we first scale sin(x) by some easily-reversible function
in order to make the result have less dynamic range. For example, if we scale sin(x) by
[8/(3 Pi) (x) - 8/(3 Pi^3) (x)^3]^-1, we get something that looks like the below plot.
</p>

{{ show_image(page.images["scaled_sine.svg"], caption="Scaled sine function with a dynamic range of ~2, plotted from (-Pi, Pi).") }}

<p>
The advantage of this is that if we optimize the <em>absolute</em> error of the above function to 2^-24 / 2, then we can apply the inverse scaling function and obtain a sin(x) approximation that's accurate to a <em>relative</em> error of 2^-24 everywhere.
I derived the scaling function by solving the min-degree odd polynomial that has a zero at x=0 and x=Pi, and a one at x=Pi/2.
</p>

<p>
Let's now project our scaled sine function onto the Chebyshev basis polynomials.
To do this, I followed pages 7-8 of {{ offsite_link("http://www.mhtl.uwaterloo.ca/courses/me755/web_chap6.pdf", "this") }} University of Waterloo pdf.

Specifically, it shows the following property for Chebyshev functions, which arise from their
orthogonality.
</p>

{{ show_image(page.images["cheby_proj1.png"], caption="Chebyshev inner-product equality") }}

<p>
If our scaled sine function can be represented in terms of the Chebyshev basis functions (it can),
i.e. f(x) = \sum_n A_n T_n(x), then the integral
\int_{-1}^1 \frac{T_n(x) f(x)}{\sqrt{1-x^2}} dx
is exactly equal to
\int_{-1}^1 \frac{T_n(x) A_n T_n(x)}{\sqrt{1-x^2}} dx

When we move A_n outside the integral, we find that the value of the integral is known from the previous equation.
Hence, we have a way to solve for each A_n and thereby re-express our scaled sine function
in terms of the Chebyshev basis.
</p>

<p>
I compute the first 9 terms of the series with the following Mathematica code.
I omit solving for A_1, A_3, A_5, and A_7 because they are zero.
In order to keep the function's domain as [-1, 1], I scale the parameter to sin(x) by Pi.
This way, we get coefficients to a function that computes one full cycle of sin(Pi x).
I'll undo the x scaling later.
</p>

{{ show_image(page.images["solve_cheb_coeffs.png"], caption="Solve for the Chebyshev coefficients to the scaled sine function. WorkingPrecision needs to be raised for NIntegrate to converge (or just use Integrate, if you're more patient).") }}

<p>
Now that we have our Chebyshev coefficients, we can reconstruct an approximation to sin(x), and also undo the scaling of the x parameter:
</p>

{{ show_image(page.images["sine_from_coeffs.png"], caption="Reconstructing sine(x) = A_0 T_0(x) + A_1 T_1(x) + ...") }}


<p>
A quick plot of the reconstructed sine(x) function looks promising:
</p>

{{ show_image(page.images["reconstructed_sine_plot.svg"], caption="Plot of reconstructed sine(x) approximation from (-Pi, Pi).") }}

<p>
Likewise, the plot for our relative error measurement: (sine[x] - Sin[x])/Sin[x].
</p>

{{ show_image(page.images["relative_error_plot.svg"], caption="Relative error of our sine(x) approximation from (-Pi, Pi).") }}

<p>
The error is very uniform and it reaches the target of 2^-25!
For completeness, here's the polynomial approximation. Chebyshev functions are themselves polynomials,
so expanding the function gives an ordinary polynomial.
</p>

{{ show_image(page.images["sine_as_poly.png"], caption="Polynomial approximation to sin(x).") }}





{{ heading("Evaluating the polynomial approximation") }}

<p>
We have our polynomial coefficients; to evaluate the polynomial, we can just use Horner's method (aka "nested multiplication"):
</p>

{{ multiline_code("""
fn horner_sine_13(x: f32) -> f32 {
    let coeffs = [
        0.999999999973088f32, // x
        -0.1666666663960699f32, // x^3
        0.00833333287058762f32, // x^5
        -0.0001984123883227529f32, // x^7,
        2.755627491096882e-6f32, // x^9
        -2.503262029557047e-8f32, // x^11
        1.58535563425041e-10f32, // x^13
    ];
    let p13 = coeffs[6];
    let p11 = p13*x*x + coeffs[5];
    let p9  = p11*x*x + coeffs[4];
    let p7  = p9*x*x  + coeffs[3];
    let p5  = p7*x*x  + coeffs[2];
    let p3  = p5*x*x  + coeffs[1];
    let p1  = p3*x*x  + coeffs[0];
    p1*x
}
""") }}

<p>
So how does this perform?
</p>

{{ show_image(page.images["horner_ulp.svg"], caption="Average ULP error v.s. x") }}

<p>
Not very well, but why?
</p>

<p>
It turns out we've reached the point where the low-error approximation is only accurate
when evaluated with infinite precision. When we perform the computations using 32-bit floats,
we're plagued by rounding errors. You see that first coefficient: 0.999999999973088f32?
</p>

{{ multiline_code("""
> println!(\"{}\", 0.999999999973088f32 == 1.0f32);
true
""") }}

<p>
Yup, we chose coefficients that can't even be represented in our number system.
Of course the results have error.
As for what we can do about it, notice <em>where</em> the error occurs.
It only becomes excessive as x approaches +/- pi, at which point sin(x) <em>should</em> approach 0.
But the slight bit of error in the coefficients makes it so it doesn't approach 0 at quite the right place.
Our polynomial already has a factor of x explicitly factored in order to create a root at x=0.
My instincts are to pull out a factor of (x-Pi) and (x+Pi) as well.
</p>


{{ show_image(page.images["fact_sine_as_poly.png"], caption="sin(x) approximation divided by (x-Pi)(x+Pi).") }}

<p>
Now if I multiplied by (x+Pi)(x-Pi) <em>in 32-bit float</em> to undo the division, that wouldn't be quite right, because the float
closest to Pi isn't exactly equal to the true value of Pi. The result is that sin(f32::consts::PI) <em>shouldn't be 0</em>,
so we don't want a root at (x-f32::consts::PI).
</p>

<p>
Instead, I'll make a root at (x+Pi+delta), where Pi is the closest f32 to the real pi, and delta is a small
correction factor that's approximately the true Pi minus the f32 Pi. Because the correction factor is close to 0,
the net value being subtracted is true to more decimal places.
</p>

{{ multiline_code("""
let coeffs = [
    -0.101321183346709072589001712988183609230944236760490476f32, // x
     0.00662087952180793343258682906697112938547424931632185616f32, // x^3
    -0.000173505057912483501491115906801116298084629719204655552f32, // x^5
     2.52229235749396866288379170129828403876289663605034418e-6f32, // x^7
    -2.33177897192836082466066115718536782354224647348350113e-8f32, // x^9
     1.32913446369766718120324917415992976452154154051525892e-10f32, // x^11
];
let x2 = x*x;
let p11 = coeffs[5];
let p9  = p11*x2 + coeffs[4];
let p7  = p9*x2  + coeffs[3];
let p5  = p7*x2  + coeffs[2];
let p3  = p5*x2  + coeffs[1];
let p1  = p3*x2  + coeffs[0];
(x - 3.1415927410125732f32 + 0.00000008742277657347586f32) *
(x + 3.1415927410125732f32 - 0.00000008742277657347586f32) * p1 * x
""") }}

{{ show_image(page.images["factored_ulp_no_invpi.svg"], caption="Average ULP error v.s. x. Max error is 5 ULP, at x=2.6183214.") }}

<p>
And there you have it. Some clever trickery could probably reduce the maximum error further, but it's not worth it to me. Knocking the maximum error down from 7 ULP, to 6 ULP to 5 ULP
took me a good 3-4 hours, and the offending values were clustered in a different area each time.
If I needed to reduce it even further, I would search for ways to address the fact that these polynomial coefficients aren't machine representable:
either a brute-force search near these coefficients, or locking them in one at a time and recomputing the remaining ones for a least-error fit.
But for my purposes, there's no shame in 5 ULP.
</p>


{% endblock %}


