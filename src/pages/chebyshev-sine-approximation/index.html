{% from 'base.html' import heading, offsite_link, show_image %}
{{ page.set_title("Approximating sin(x) to 6 ULP with Chebyshev polynomials") }}

{% extends 'blog_entry.html' %}

{% block entry_content %}

{{ multiline_code("""
fn sine(x: f32) -> f32 {
    let coeffs = [
        -0.99999999708226169269357792387896077290175440567272155f32, // x
         0.0653454616675179879325123271391979732993359723450347664f32, // x^3
        -0.00171242628318431121149246551606740816920148478738367486f32, // x^5
         0.0000248940277523565270075164091811365414423262202769440752f32, // x^7
        -2.30137360037117700001456783725981576185678955642394186e-7f32, // x^9
         1.31180313525500403649581979815056543944503621987677802e-9f32, // x^11
    ];
    let x2 = x*x;
    let p11 = coeffs[5];
    let p9  = p11*x2 + coeffs[4];
    let p7  = p9*x2  + coeffs[3];
    let p5  = p7*x2  + coeffs[2];
    let p3  = p5*x2  + coeffs[1];
    let p1  = p3*x2  + coeffs[0];
    p1*x * 
    (x - 3.1415927410125732f32 + 0.00000008742277657347586f32)*0.3183098861837907f32 *
    (x + 3.1415927410125732f32 - 0.00000008742277657347586f32)*0.3183098861837907f32
}
""") }}
<p>
That code above will compute the sine of any IEEE single-precision float in the range of (-Pi, Pi)
to within 6 Units of Least Precision ({{ offsite_link("https://en.wikipedia.org/wiki/Unit_in_the_last_place", "ULP") }}).
A ULP is a useful way to measure relative error in a floating point computation. Given two adjacent floating point numbers, their difference
in terms of ULP is essentially equivalent to the number of floats that lie between them (plus one).
</p>

<p>
Given that IEEE single-precision floats have 24 bits of precision, this sine function is accurate to within 0.000036% (i.e. 6/2^24).
Achieving this precision over a domain <em>this</em> large was a challenge for me.
Many polynomial approximations to sin(x) operate only over (-Pi/4, Pi/4) or (-Pi/2, Pi/2).
These methods require modifications to work with larger ranges that I had not seen published before,
so the remainder of this article will discuss my approach to deriving the sine(x) function above.
</p>

{{ heading("Sin(x) as a sum of Chebyshev polynomials") }}

<p>
The first step is to re-express sine(x) over the domain of interest as an infinite polynomial.
One could use a Taylor series, but convergence is very slow.
Instead, we use Chebyshev polynomials.
</p>

<p>
For me, it helps to view these things using concepts from linear algebra.
Any continuous and smooth function can be expressed as a polynomial.
P(x) = a_0 + a_1 x + a_2 x^2 + ...

This is a weighted sum of powers of x, and because {1, x, x^2, ...} are all orthogonal,
we can think of this set as a <em>basis</em> for our function.
</p>

<p>
Instead of representing our function in terms of the most familiar basis for polynomials ({1, x, x^2, ...}),
we can instead use {T_0(x), T_1(x), T_2(x), ...}, where T_n(x) is the
{{ offsite_link("https://en.wikipedia.org/wiki/Chebyshev_polynomials#First_kind", "Chebyshev polynomial of the first kind") }}.
</p>

{{ show_image(page.images["chebyshev_basis.svg"], caption="The first five Chebyshev polynomials. From Wikipedia (public domain).") }}

<p>
Chebyshev polynomials have the property that all their local extrema lie within -1.0 < x < 1.0, and each such extrema
is at exactly +/- 1.
For those familiar with Fourier series, this should look somewhat familiar. We're projecting our sin(x) function onto
a basis where each basis function is oscillating at an increasing rate.
It's intuitive, then, that the smoother our function is, the more rapidly the coefficients of its projection
onto {T_0(x), T_1(x), ...} will decay (e.g. sin(x) doesn't oscillate rapidly over the interval (-1, 1), so any
rapidly-oscillating T_n(x) component would have to be small).

Although our basis has infinite cardinality, most energy is located in the lower components.
We can truncate the basis to {T_0(x), ... T_N(x)} for a relatively small value of N and achieve
a <em>very</em> good approximation of sin(x) over the interval (-1.0, 1.0).
</p>

<p>
Notice that for a function approximated with a finite set of Chebyshev polynomials,
the error is spread in a fairly uniform manner. The approximation isn't intrinsically more accurate
near 0 and less accurate at the extremeties, for example. We can say that
|E(x)| < B for all -1 < x < 1, where E(x) is the error of our approximation function (E(x) = sin^(x) - sin(x)),
and B is some bound that decreases as we increase the number of terms in our approximation.
</p>

<p>
This is good. If we approximated sin(x) with 6 chebyshev terms, we might well get
the error bound, B, down to 10^-9. However, optimizing for <em>absolute</em> error is generally
not what we want! The nature of floating point numbers is such that precision
is as much as 10^-45 near zero, and as little as 10^-8 near one.
What we really want to minimize is the <em>relative error</em>. As long as
|E(x) / sin(x)| < 2^-24, we know that there is no closer float to the true value than what
we've approximated*.
</p>

<p>
* The catch is that achieving |E(x) / sin(x)| < 2^-24 only ensures we're off by less than 1 ULP.
If the true answer falls in between two floats, this isn't enough to guarantee that we round to the
correct one. If the answer is nearly the average of the two adjacent floats,
being off by just 0.01 ULP in our model could cause incorrect rounding that actually bumps us to a
full 0.5 ULP error. I imagine optimizing for 0.5 ULP error requires more novel techniques.
</p>


{% endblock %}

