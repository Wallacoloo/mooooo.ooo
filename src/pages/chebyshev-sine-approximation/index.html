{% from 'base.html' import heading, offsite_link, show_image %}
{{ page.set_title("Approximating sin(x) to 5 ULP with Chebyshev polynomials") }}
{{ page.offsite_comments("https://news.ycombinator.com/item?id=14328650") }}

{# Note:
def tof32(f):
...     return struct.unpack('>f', struct.pack('>f', f))[0]
#}

{% extends 'blog_entry.html' %}

{% block entry_content %}

{{ multiline_code("""
fn sine(x: f32) -> f32 {
    let coeffs = [
        -0.10132118f32,          // x
         0.0066208798f32,        // x^3
        -0.00017350505f32,       // x^5
         0.0000025222919f32,     // x^7
        -0.000000023317787f32,   // x^9
         0.00000000013291342f32, // x^11
    ];
    let pi_major = 3.1415927f32;
    let pi_minor = -0.00000008742278f32;
    let x2 = x*x;
    let p11 = coeffs[5];
    let p9  = p11*x2 + coeffs[4];
    let p7  = p9*x2  + coeffs[3];
    let p5  = p7*x2  + coeffs[2];
    let p3  = p5*x2  + coeffs[1];
    let p1  = p3*x2  + coeffs[0];
    (x - pi_major - pi_minor) *
    (x + pi_major + pi_minor) * p1 * x
}
""", "rust") }}
<p>
That code above will compute the sine of any IEEE single-precision float in the range of {{"(-\\pi, \\pi)"|tex}}
to within 4.58 Units of Least Precision ({{ offsite_link("https://en.wikipedia.org/wiki/Unit_in_the_last_place", "ULP") }}).
A ULP is a useful way to measure relative error in a floating point computation. Given two adjacent floating point numbers, their difference
in terms of ULP is essentially equivalent to the number of floats that lie between them (plus one).
</p>

<p>
Given that IEEE single-precision floats have 24 bits of precision, this sine function is accurate to within 0.000027% (i.e. {{"4.58/2^{24}"|tex}}).
Achieving this precision over a domain <em>this</em> large was a challenge for me.
Many polynomial approximations to {{"\\sin(x)"|tex}} operate only over {{"(-\\pi/4, \\pi/4)"|tex}} or {{"(-\\pi/2, \\pi/2)"|tex}}
and use special properties of the sine function to reduce their arguments into this range (e.g. {{"\\sin(x+\\pi) = -\\sin(x)"|tex}} to force arguments into {{"(-\\pi/2, \\pi/2)"|tex}}).
To work over larger ranges without reduction (e.g. if the input is known to be bounded by {{"(-\\pi, \\pi)"|tex}}),
these methods require modifications that I had not seen published before,
so the remainder of this article will discuss my approach to deriving the sine function above.
</p>

<p>
When I need to distinguish between the approximation of {{"\\sin(x)"|tex}} and the true {{"\\sin(x)"|tex}},
I'll be using {{"\\widehat{\\sin}(x)"|tex}} to represent the approximation.
</p>


{{ heading("Sin(x) as a sum of Chebyshev polynomials") }}

<p>
The first step is to re-express {{"\\sin(x)"|tex}} over the domain of interest as an infinite polynomial.
One could use a Taylor series, but convergence is very slow.
Instead, I use Chebyshev polynomials.
</p>

<p>
For me, it helps to view these things using concepts from linear algebra.
Any continuous and smooth function can be expressed as a polynomial:
{{"P(x) = a_0 + a_1 x + a_2 x^2 + \\dots"|tex}}.

This is a weighted sum of powers of {{"x"|tex}}, and because {{"\\{1, x, x^2, \\dots\\}"|tex}} are all orthogonal,
we can think of this set as a <em>basis</em> for our function.
</p>

<p>
Instead of representing our function in terms of the standard polynomial basis ({{"\\{1, x, x^2, \\dots\\}"|tex}}),
we can use {{"\\{T_0(x), T_1(x), T_2(x), \\dots\\}"|tex}}, where {{"T_n(x)"|tex}} is the
{{ offsite_link("https://en.wikipedia.org/wiki/Chebyshev_polynomials#First_kind", "Chebyshev polynomial of the first kind") }}.
</p>

{{ show_image(page.images["chebyshev_basis.svg"], caption="The first five Chebyshev polynomials. From Wikipedia (public domain).") }}

<p>
Chebyshev polynomials have the property that all their local extrema lie within {{"-1 < x < 1"|tex}}, and each such extrema
has a value of exactly {{"\\pm 1"|tex}}.
For those familiar with Fourier series, this should look somewhat familiar. We're projecting the sine function onto
a basis where each basis function is oscillating at an increasing rate.
It's intuitive, then, that the smoother a function is, the more rapidly the coefficients of its projection onto
{{"\\{T_0(x), T_1(x), \\dots\\}"|tex}} will decay (e.g. {{"\\sin(x)"|tex}} doesn't oscillate rapidly over the interval {{"(-1, 1)"|tex}}, so any
rapidly-oscillating {{"T_n(x)"|tex}} component would have to be small).

Although the basis has infinite cardinality, most energy is located in the lower components.
We can truncate the basis to {{"\\{T_0(x), \\dots, T_N(x)\\}"|tex}} for a relatively small value of N and achieve
a <em>very</em> good approximation of {{"\\sin(x)"|tex}} over the interval {{"(-1, 1)"|tex}}.
</p>

<p>
Note that for a smooth function approximated with a finite set of Chebyshev polynomials,
the error is spread in a fairly uniform manner. The approximation isn't intrinsically more accurate
near 0 and less accurate at the extremeties, for example. We can say that
{{"|E(x)| < B"|tex}} for all {{"-1 < x < 1"|tex}}, where {{"E(x)"|tex}} is the error of the approximation function 
({{"E(x) = \\widehat{\\sin}(x) - \\sin(x)"|tex}}),
and {{"B"|tex}} is some bound that decreases as we increase the number of terms in the approximation.
</p>

<p>
This is good. If we approximated {{"\\sin(x)"|tex}} with 6 chebyshev terms, we might well get
the error bound, {{"B"|tex}}, down to {{"10^{-9}"|tex}}. However, optimizing for <em>absolute</em> error is generally
not what we want! The nature of floating point numbers is such that precision
is as much as {{"10^{-45}"|tex}} near zero, and as little as {{"10^{-8}"|tex}} near one.
What we really want to minimize is the <em>relative error</em>. As long as
{{"|E(x) / \\sin(x)| < 2^{-24}"|tex}}, we know that there is no closer float to the true value than what
we've approximated*.
</p>

<p>
* The catch is that achieving {{"|E(x) / \\sin(x)| < 2^{-24}"|tex}} only ensures we're off by less than 1 ULP <em>sans rounding</em>.
If the true answer falls in between two floats, this isn't enough to guarantee that we round to the
correct one. If the answer is nearly the average of the two adjacent floats,
being off by just 0.01 ULP in our model could cause incorrect rounding that actually bumps us to a
full 0.5 ULP error. I imagine optimizing for 0.5 ULP error requires more novel techniques.
I found a {{ offsite_link("http://www.cl.cam.ac.uk/~jrh13/papers/fmcad00.pdf", "paper") }} detailing
Intel's formally-verified sine implementation for their IA-64 architecture, and even it
only claims accuracy to 0.57 ULP.
</p>

{# TODO: Even intel only advertizes something like 0.56 ULP, right? Would be an interesting aside.
This [intel] paper shows the derivation of 0.57341 ULPs result over (-2^64, 2^64),
formally proved: http://www.cl.cam.ac.uk/~jrh13/papers/fmcad00.pdf
#}

{{ heading("Optimizing relative error") }}
<p>
To optimize for relative error, we first scale {{"\\sin(x)"|tex}} by some easily-reversible function
in order to make the result have less dynamic range. For example, if we scale {{"\\sin(x)"|tex}} by
{{"[\\frac{8}{3\\pi}(x) - \\frac{8}{3\\pi^3}(x^3)]^{-1}"|tex}},
we get something that looks like the below plot.
</p>

{{ show_image(page.images["scaled_sine.svg"], caption="Scaled sine function with a dynamic range of ~2, plotted on " + "(-\\pi, \\pi)"|tex + ".") }}

<p>
The advantage of this is that if we optimize the <em>absolute</em> error of the above function to {{"2^{-24}/d"|tex}}, where {{"d"|tex}} is the dynamic range,
then we can apply the inverse scaling function and obtain a {{"\\sin(x)"|tex}} approximation that's accurate to a <em>relative</em>
error of {{"2^{-24}"|tex}} everywhere.
I derived the scaling function by solving the min-degree odd polynomial that has a zero at {{"x=0"|tex}} and {{"x=\\pi"|tex}}, and a one at {{"x=\\pi/2"|tex}}.
</p>

<p>
Let's now project the scaled sine function onto the Chebyshev basis polynomials.
To do this, I followed pages 7-8 of {{ offsite_link("http://www.mhtl.uwaterloo.ca/courses/me755/web_chap6.pdf", "this") }} University of Waterloo pdf.

Specifically, it shows the following property for Chebyshev functions, which arises from their
orthogonality.
</p>

{{ show_image(page.images["cheby_proj1.png"], caption="Chebyshev inner-product equality") }}

<p>
If the scaled sine function is representable in terms of the Chebyshev basis functions,
i.e. {{"f(x) = \\sum_n A_n T_n(x)"|tex}}, then the integral
{{"\\int_{-1}^1 \\frac{T_n(x) f(x)}{\\sqrt{1-x^2}} dx"|tex}}
is exactly equal to
{{"\\int_{-1}^1 \\frac{T_n(x) A_n T_n(x)}{\\sqrt{1-x^2}} dx"|tex}}.

By moving {{"A_n"|tex}} out of the integral,
the following relation is obtained: {{"\\langle f(x), T_n(x) \\rangle = A_n \\langle T_n(x), T_n(x) \\rangle"|tex}}, where {{"\\langle g(x), h(x) \\rangle"|tex}} represents the inner product, i.e. {{"\\langle g(x), h(x) \\rangle = \\int_{-1}^1 \\frac{g(x)h(x)}{\\sqrt{1-x^2}}dx"|tex}}.

This gives a straightforward way to solve for each {{"A_n"|tex}} and thereby re-express the scaled sine function
in terms of the Chebyshev basis.
</p>

<p>
I compute the first 11 terms of the series with the following Mathematica code.
I omit solving for {{"A_1, A_3, A_5, A_7"|tex}} and {{"A_9"|tex}} because symmetry requires they be zero.
In order to keep the function's domain as {{"(-1, 1)"|tex}}, I scale the parameter to {{"\\sin(x)"|tex}} by {{"\\pi"|tex}}.
This way, we get coefficients to a function that computes one full cycle of {{"\\sin(\\pi x)"|tex}}.
I'll undo the scaling of {{"x"|tex}} later.
</p>

{{ show_image(page.images["solve_cheb_coeffs.png"], caption="Solve for the Chebyshev coefficients to the scaled sine function. WorkingPrecision needs to be raised for NIntegrate to converge (or use Integrate, if you're patient).") }}

<p>
Now that we have the Chebyshev coefficients, we can reconstruct an approximation to {{"\\sin(x)"|tex}} and also undo the scaling of the {{"x"|tex}} parameter:
</p>

{{ show_image(page.images["sine_from_coeffs.png"], caption="Reconstructing " + "\\widehat{\\sin}(x) = A_0 T_0(x) + A_1 T_1(x) + \\dots"|tex) }}


<p>
A quick plot of the reconstructed sine function looks promising:
</p>

{{ show_image(page.images["reconstructed_sine_plot.svg"], caption="Plot of reconstructed sine approximation on " + "(-\\pi, \\pi)"|tex + ".") }}

<p>
Likewise, the plot for the relative error measurement: {{"(\\widehat{\\sin}(x) - \\sin(x))/\\sin(x)"|tex}}.
</p>

{{ show_image(page.images["relative_error_plot.svg"], caption="Relative error of the sine approximation on " + "(-\\pi, \\pi)"|tex + ".") }}

<p>
The error is fairly uniform, and it reaches the target of {{"2^{-24}"|tex}}.
For completeness, the polynomial coefficients are shown below. Chebyshev functions are themselves polynomials,
so expanding the function gives an ordinary polynomial.
</p>

{# Not actually true: our process doesn't directly optimize relative error.
Note that the error could be reduced a bit further by using an iterative process like the
{{ offsite_link("https://en.wikipedia.org/wiki/Remez_algorithm", "Remez Exchange Algorithm") }},
which forces all the peaks to be equal.
#}

{{ show_image(page.images["sine_as_poly.png"], caption="Polynomial approximation to " + "\\sin(x)"|tex + ".") }}





{{ heading("Evaluating the polynomial approximation") }}

<p>
Given these coefficients, the polynomial can be evaulated with Horner's method (aka "nested multiplication"):
</p>

{{ multiline_code("""
fn sine(x: f32) -> f32 {
    let coeffs = [
        0.999999999973088f32, // x
        -0.1666666663960699f32, // x^3
        0.00833333287058762f32, // x^5
        -0.0001984123883227529f32, // x^7,
        2.755627491096882e-6f32, // x^9
        -2.503262029557047e-8f32, // x^11
        1.58535563425041e-10f32, // x^13
    ];
    let p13 = coeffs[6];
    let p11 = p13*x*x + coeffs[5];
    let p9  = p11*x*x + coeffs[4];
    let p7  = p9*x*x  + coeffs[3];
    let p5  = p7*x*x  + coeffs[2];
    let p3  = p5*x*x  + coeffs[1];
    let p1  = p3*x*x  + coeffs[0];
    p1*x
}
""", "rust") }}

<p>
So how does this perform?
</p>

{{ show_image(page.images["horner_ulp.svg"], caption="Average error (in ULP) v.s. x.") }}

<p>
Not very well, but why?
</p>

<p>
It turns out we've reached the point where the low-error approximation is only accurate
when evaluated with infinite precision. When we perform the computations using 32-bit floats,
we're plagued by rounding errors. You see that first coefficient: 0.999999999973088f32?
</p>

{{ multiline_code("""
> println!(\"{}\", 0.999999999973088f32 == 1.0f32);
true
""", "rust") }}

<p>
Yup, we chose coefficients that can't even be represented in our number system.
Of course the results have error.
As for what we can do about it, notice <em>where</em> the error occurs.
It only becomes excessive as {{"x"|tex}} approaches {{"\\pm \\pi"|tex}}, at which point {{"\\widehat{\\sin}(x)"|tex}} <em>should</em> approach 0.
But the slight bit of error in the coefficients makes it so it doesn't approach 0 at quite the right place.
Our polynomial already has a factor of {{"x"|tex}} explicitly factored in order to create a root at {{"x=0"|tex}}.
My instincts are to pull out a factor of {{"(x-\\pi)"|tex}} and {{"(x+\\pi)"|tex}} as well.
</p>


{{ show_image(page.images["fact_sine_as_poly.png"], caption="\\sin(x)"|tex + " approximation divided by " + "(x-\\pi)(x+\\pi)"|tex + ".") }}

<p>
Now if I multiplied by {{"(x+\\pi)(x-\\pi)"|tex}} <em>in 32-bit float</em> to undo the division, that wouldn't be quite right, because the float
closest to {{"\\pi"|tex}} isn't exactly equal to the true value of {{"\\pi"|tex}}. The result is that sin(f32::consts::PI) <em>shouldn't be 0</em>,
so we don't want a root at (x-f32::consts::PI).
</p>

<p>
Instead, I'll make a root at {{"x+\\hat{\\pi}+\\Delta"|tex}}, where {{"\\hat{\\pi}"|tex}} is the closest f32 to the real {{"\\pi"|tex}}, and {{"\Delta"|tex}} is a small
correction factor that's approximately the true {{"\\pi"|tex}} minus the f32 {{"\\hat{\\pi}"|tex}}. Because the correction factor, {{"\Delta"|tex}}, is close to 0,
the net value being added/subtracted is accurate to more decimal places. The effect is only seen when x is near {{"\\pm \\pi"|tex}}, otherwise the {{"\Delta"|tex}} offset
itself gets absorbed by rounding errors. But that's ok, because it's only needed to fix errors near {{"\\pm \\pi"|tex}} in the first place.
</p>

{{ multiline_code("""
let coeffs = [
    -0.101321183346709072589001712988183609230944236760490476f32, // x
     0.00662087952180793343258682906697112938547424931632185616f32, // x^3
    -0.000173505057912483501491115906801116298084629719204655552f32, // x^5
     2.52229235749396866288379170129828403876289663605034418e-6f32, // x^7
    -2.33177897192836082466066115718536782354224647348350113e-8f32, // x^9
     1.32913446369766718120324917415992976452154154051525892e-10f32, // x^11
];
let x2 = x*x;
let p11 = coeffs[5];
let p9  = p11*x2 + coeffs[4];
let p7  = p9*x2  + coeffs[3];
let p5  = p7*x2  + coeffs[2];
let p3  = p5*x2  + coeffs[1];
let p1  = p3*x2  + coeffs[0];
(x - 3.1415927410125732f32 + 0.00000008742277657347586f32) *
(x + 3.1415927410125732f32 - 0.00000008742277657347586f32) * p1 * x
""", "rust") }}

{{ show_image(page.images["factored_ulp.svg"], caption="Average error (in ULP) v.s. x.") }}

<p>
At this point, it helps to look at the error bounds, rather than just their average magnitude:
</p>

{{ show_image(page.images["factored_ulp_bounds.svg"], caption="Max & min error (signed) v.s. x. The global maximum error is 4.90 ULP, at x=2.6183214.") }}

<p>
The plot above shows that the error oscillates over a large range, and rapidly.
The plot was made by taking the min/max signed error over each group of 4096 adjacent floats, so we see that the signed error changes by
as much as 9 ULP over the course of just 4096 adjacent floats. It doesn't seem that the error could be meaningfully addressed by adding more polynomial terms
or better optimization of the existing polynomial coefficients, then -
it's essentially noise.
</p>

<p>
Without resorting to piecewise functions (or another form of control logic), I think my options are fairly limited at this point. 
I could use a similar trick as with subtracting {{"\\pi"|tex}}, and represent each coefficient more precisely
as a sum of two floats with differing magnitude (i.e. the f32 analog of
{{offsite_link("https://en.wikipedia.org/wiki/Quadruple-precision_floating-point_format#Double-double_arithmetic", "double doubles") }}),
but at the cost of more complexity.
Or I could brute-force search nearby sets of coefficients and see if there are any that just <em>happen</em> to be
less susceptible to this noise.
</p>

<p>
So I ran the brute-forcer for 20 hours and it turned up a slightly better result (it actually got this result within the first hour).
The resulting coefficients are the ones shown in the code at the top of this article.
The maximum error over {{"(-\\pi, \\pi)"|tex}} is now 4.58 ULP and occurs at x=3.020473.
</p>

{{ show_image(page.images["bruteforce_ulp_bounds.svg"], caption="Max & min error v.s. x for the brute-forced coefficients.") }}


{# Brute-force search reveals:
ULP>=5, total 118849126: [-0.101321176, 0.0066208793, -0.00017350508, 0.000002522292, -0.000000023317789, 0.00000000013291343]
ULP>=4, total 3040487690: [-0.10132118, 0.00662088, -0.0001735051, 0.0000025222919, -0.000000023317789, 0.0000000001329134]

NEW ULP>=4.720577212050557, total 3051743422.722066: [-0.10132118, 0.0066208798, -0.00017350506, 0.0000025222926, -0.00000002331779, 0.00000000013291343]
factored_horner: 4.584126954898238 max ULP error at -3.020473 with 3051951143.2803235 summed errors
#}

<p>
Although performance wasn't my goal here, I'll conclude with a benchmark against the standard library's implementation,
which probably handles infinities, NaNs, and performs reduction for numbers outside {{"(-\\pi, \\pi)"|tex}}.
</p>

{{ multiline_code("""
extern crate rand;
extern crate test;
use rand::{Rng, XorShiftRng};
use test::Bencher;
const TWO_PI: f32 = 2f32*std::f32::consts::PI;

#[bench]
fn bench_my_sin(b: &mut Bencher) {
    let mut rng = XorShiftRng::new_unseeded();
    b.iter(|| {
        let x = (rng.next_f32()-0.5f32) * TWO_PI;
        bruteforce_sine(x)
    });
}

#[bench]
fn bench_f32_sin(b: &mut Bencher) {
    let mut rng = XorShiftRng::new_unseeded();
    b.iter(|| {
        let x = (rng.next_f32()-0.5f32) * TWO_PI;
        x.sin()
    });
}
""", "rust") }}

<br>

{{ multiline_code("""
$ cargo bench
test bench_f32_sin ... bench:          16 ns/iter (+/- 1)
test bench_my_sin  ... bench:           7 ns/iter (+/- 1)
""", "shell") }}

{% endblock %}


